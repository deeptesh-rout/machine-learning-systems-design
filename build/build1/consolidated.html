<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8">
    <title>Machine learning systems design</title>
    
    <link rel="stylesheet" href="assets/pdf.css">

  </head>
  <body data-type="book">
    <h1>Machine Learning Interviews</h1>
<h2>Machine Learning Systems Design</h2>
<b>Chip Huyen</b><br>
<a href="https://huyenchip.com">huyenchip.com</a><br>
<a href="https://twitter.com/chipro">@chipro</a>
<nav data-type='toc'>
  <h3>Table of Contents</h3>
  <ol>
<li>
    <a data-toc="chapter" href="#introduction-qzZkHeP">Introduction</a>
    <ol>
<li>
    <a data-toc="sect1" href="#research-vs-production-8DpYHKz">Research vs production</a>
    <ol>
<li>
    <a data-toc="sect2" href="#performance-requirements-5DWpHYz">Performance requirements</a>
  </li>
<li>
    <a data-toc="sect2" href="#compute-requirements-eR1mHdR">Compute requirements</a>
  </li>
  </ol>
</li>
  </ol>
</li>
<li>
    <a data-toc="chapter" href="#design-a-machine-learning-system-dwGQI5R">Design a machine learning system</a>
    <ol>
<li>
    <a data-toc="sect1" href="#project-setup-zlkQIG9">Project setup</a>
  </li>
<li>
    <a data-toc="sect1" href="#data-pipeline-698WI4R">Data pipeline</a>
  </li>
<li>
    <a data-toc="sect1" href="#modeling-o97BIGR">Modeling</a>
    <ol>
<li>
    <a data-toc="sect2" href="#model-selection-eRQEIDR">Model selection</a>
  </li>
<li>
    <a data-toc="sect2" href="#training-5RLqIW9">Training</a>
    <ol>
<li>
    <a data-toc="sect3" href="#debugging-89pbIkl">Debugging</a>
  </li>
<li>
    <a data-toc="sect3" href="#hyperparameter-tuning-BlADIyw">Hyperparameter tuning</a>
  </li>
<li>
    <a data-toc="sect3" href="#scaling-49BpIQl">Scaling</a>
  </li>
  </ol>
</li>
  </ol>
</li>
<li>
    <a data-toc="sect1" href="#serving-091rIYw">Serving</a>
  </li>
  </ol>
</li>
<li>
    <a data-toc="chapter" href="#case-studies-bYrWS80">Case studies</a>
  </li>
<li>
    <a data-toc="chapter" href="#exercises-rWl8SQW">Exercises</a>
  </li>
  </ol>
</nav>

<p style="page-break-before: always"></p>
<section data-type="chapter" id="introduction-qzZkHeP"><h1>Introduction</h1>
<p>This part contains 27 open-ended questions that test your ability to put together what you&apos;ve learned to design systems to solve practical problems. Interviewers give you a problem, possibly related to their products, and ask you to design a machine learning system to solve it. This type of question has become so popular that it&apos;s almost guaranteed that you&apos;ll be asked at least one during your interview process. In an hour-long interview, you might have time to go over only one or two questions.</p>
<p>These questions don&apos;t have single correct answers, though there are answers that are considered correct. There are many ways to solve a problem, and there are many follow-up questions the interviewer can ask to evaluate the candidate&apos;s knowledge, implementation ability, and critical thinking skills. Interviewers generally agree that even if you can&apos;t get to a working solution, as long as you communicate your thinking process to show that you understand different constraints, trade-offs, and concerns of your system, it&apos;s good enough.</p>
<p>These are the kind of questions candidates often both love and hate. Candidates love these questions because they are fun, practical, flexible, and require the least amount of memoization. Candidates hate these questions for several reasons.</p>
<p>First, they lack evaluation guidelines. It&apos;s frustrating for candidates when the interviewer asks an open-ended question but expects only one right answer -- the answer that the interviewer is familiar with. It&apos;s hard to come up with a perfect solution on the spot and candidates might need help overcoming obstacles. However, many interviewers are quick to dismiss candidates&apos; half-formed solutions because they don&apos;t see where the solutions are headed.</p>
<p>Second, these questions are ambiguous. There&apos;s no typical structure for these interviews. Each interview starts with a purposefully vague task: design X. It&apos;s your job as the candidate to ask for clarification and narrow down the problem. You drive the interview and choose what to focus on. What you choose to focus on speaks volumes about your interest, your experience, and your understanding of the problem.</p>
<p>Many candidates don&apos;t even know what a good answer looks like. It&apos;s not taught in school. If you&apos;ve never deployed a machine learning system to users, you might not even know what you need to worry about when designing a system.</p>
<p>When I asked on Twitter what interviewers look for with this type of question, I got varying answers. <a href="https://twitter.com/dkislyuk/status/1152246124960350208?s=20">Dmitry Kislyuk</a>, an engineering manager for Computer Vision at Pinterest, is more interested in the non-modeling parts:</p>
<p>&quot;<em>Most candidates know the model classes (linear, decision trees, LSTM, convolutional neural networks) and memorize the relevant information, so for me the interesting bits in machine learning systems interviews are data cleaning, data preparation, logging, evaluation metrics, scalable inference, feature stores (recommenders/rankers).</em>&quot;</p>
<p><a href="https://twitter.com/gmravi2003/status/1152284255671599104?s=20">Ravi Ganti</a>, a data scientist at WalmartLabs, looks for the ability to divide and conquer the problem:</p>
<p>&quot;<em>When I ask such questions, what I am looking for is the following. 1. Can the candidate break down the open ended problem into simple components (building blocks) 2. Can the candidate identify which blocks require machine learning and which do not.</em>&quot;</p>
<p>Similarly, <a href="https://twitter.com/ilblackdragon/status/1152648214203363330?s=20">Illia Polosukhin</a>, a co-founder of the blockchain startup NEAR Protocol and who was previously at Google and MemSQL, looks for the fundamental problem-solving skills:</p>
<p>&quot;<em>I think this [the machine learning systems design] is the most important question. Can a person define the problem, identify relevant metrics, ideate on data sources and possible important features, understands deeply what machine learning can do. Machine learning methods change every year, solving problems stays the same.</em>&quot;</p>
<p>This book doesn&apos;t attempt to give perfect answers -- they don&apos;t exist. Instead, it aims to provide a framework for approaching those questions.</p>
<section data-type="sect1" id="research-vs-production-8DpYHKz"><h1>Research vs production</h1>
<p>To approach these questions, let&apos;s first examine the fundamental differences between machine learning in an academic setting and machine learning in production.</p>
<p>In academic settings, people care more about training whereas in production, people care more about serving. Candidates who have only learned about machine learning but haven&apos;t deployed a system in the real world often make the mistake of focusing entirely on training: getting the model to do well on some benchmark task without thinking of how it would be used.</p>
<section data-type="sect2" id="performance-requirements-5DWpHYz"><h2>Performance requirements</h2>
<p>In machine learning research, there&apos;s an obsession with achieving state-of-the-art (SOTA) results on benchmarking tasks. To edge out a small increase in performance, researchers often resort to techniques that make models too complex to be useful.</p>
<p>A technique often used by the winners of machine learning competitions, including the famed $1M Netflix Prize and many Kaggle competitions, is <a href="https://en.wikipedia.org/wiki/Ensemble_learning">ensembling</a>: combining &quot;<em>multiple learning algorithms to obtain better predictive performance than could be obtained from any of the constituent learning algorithms alone.</em>&quot; While it can give you a few percentage point increase in performance, ensembling makes your system more complex, requires much more time to develop and train, and costs more.</p>
<p>A few percentage points might be a big deal on a leaderboard, but might not even be noticeable for users. From a user&apos;s point of view, an app with a 95% accuracy is not that different from an app with a 96% accuracy.
<br><br><br><br></p>
<hr>
<p><strong><em>Note</em></strong></p>
<p>There have been many arguments that leaderboard-styled competitions, especially Kaggle, aren&apos;t machine learning. An obvious argument is that Kaggle already does a lot of the steps needed for machine learning for you (<a href="https://jvns.ca/blog/2014/06/19/machine-learning-isnt-kaggle-competitions/">Machine learning isn&apos;t Kaggle competitions</a>, Julia Evans).</p>
<p>A less obvious, but fascinating, argument is that due to the multiple hypothesis testing scenario that happens when you have multiple teams testing on the same hold-out test set, a model can do better than the rest just by chance (<a href="https://lukeoakdenrayner.wordpress.com/2019/09/19/ai-competitions-dont-produce-useful-models/">AI competitions don&apos;t produce useful models</a>, Luke Oakden-Rayner, 2019).</p>
<hr>
</section><section data-type="sect2" id="compute-requirements-eR1mHdR"><h2>Compute requirements</h2>
<p>In the last decade, machine learning systems have become exponentially larger, requiring exponentially more compute power and exponentially more data to train. According to <a href="https://openai.com/blog/ai-and-compute/">OpenAI</a>, &quot;<em>the amount of compute used in the largest AI training runs has doubled every 3.5 months.</em>&quot;</p>
<p>From AlexNet in 2012 to AlphaGo Zero in 2018, the compute power required increased 300,000 times. The architectural search that resulted in AmoebaNets by the Google AutoML team required 450 K40 GPUs for 7 days (<a href="https://arxiv.org/abs/1802.01548">Regularized Evolution for Image Classifier Architecture Search</a>, Real et al., 2018). If done on one GPU, it&apos;d have taken 9 years.</p>
<center>
<img alt="AI and Compute" src="assets/ai_compute.png" style="float: center; max-width: 60%; margin: 0 0 1em 1em">
</center>
<p>These massive models make ideal headlines, not ideal products. They are too expensive to train, too big to fit onto consumer devices, and too slow to be useful to users. When I talk to companies that want to use machine learning in production, many tell me that they want to do what leading research labs are doing, and I have to explain to them that they don&apos;t.</p>
<p>There&apos;s undeniably a lot of value in fundamental research. These big models might eventually be useful as the community figures out a way to make them smaller and faster, or can be used as pretrained models on top of which consumer products are developed. However, the goals of research are very different from the goals of production. When asked by engineers to develop systems to be used in production, you need to keep the production goals in mind.</p>
</section></section></section>
<section data-type="chapter" id="design-a-machine-learning-system-dwGQI5R"><h1>Design a machine learning system</h1>
<p>Designing a machine learning system is an iterative process. There are generally four main components of the process: project setup, data pipeline, modeling (selecting, training, and debugging your model), and serving (testing, deploying, maintaining).</p>
<p>The output from one step might be used to update the previous steps. Some scenarios:</p>
<ul>
<li>After examining the available data, you realize it&apos;s impossible to get the data needed to solve the problem you previously defined, so you have to frame the problem differently.</li>
<li>After training, you realize that you need more data or need to re-label your data.</li>
<li>After serving your model to the initial users, you realize that the way they use your product is very different from the assumptions you made when training the model, so you have to update your model.</li>
</ul>
<p>When asked to design a machine learning system, you need to consider all of these components.</p>
<center>
<img alt="Machine learning project flow" src="assets/ml_project_flow.png" style="float: center; max-width: 45%; margin: 0 0 1em 1em">
</center>
<section data-type="sect1" id="project-setup-zlkQIG9"><h1>Project setup</h1>
<p>Before you even say neural network, you should first figure out as much detail about the problem as possible.</p>
<ul>
<li><strong>Goals</strong>: What do you want to achieve with this problem? For example, if you&apos;re asked to create a system to rank what activities to show first in one&apos;s newsfeed on Facebook, some of the possible goals are: to minimize the spread of misinformation, to maximize revenue from sponsored content, or to maximize users&apos; engagement.</li>
<li><strong>User experience</strong>: Ask your interviewer for a step by step walkthrough of how end users are supposed to use the system. If you&apos;re asked to predict what app a phone user wants to use next, you might want to know when and how the predictions are used. Do you only show predictions only when a user unlocks their phone or during the entire time they&apos;re on their phone?</li>
<li><strong>Performance constraints</strong>: How fast/good does the prediction have to be? What&apos;s more important: precision or recall? What&apos;s more costly: false negative or false positive? For example, if you build a system to predict whether someone is vulnerable to certain medical problems, your system must not have false negatives. However, if you build a system to predict what word a user will type next on their phone, it doesn&apos;t need to be perfect to provide value to users.</li>
<li><strong>Evaluation</strong>: How would you evaluate the performance of your system, during both training and inferencing? During inferencing, a system&apos;s performance might be inferred from users&apos; reactions, e.g. how many times they choose the system&apos;s suggestions. If this metric isn&apos;t differentiable, you need another metric to use during training, e.g. the loss function to optimize. Evaluation can be very difficult for generative models. For example, if you&apos;re asked to build a dialogue system, how do you evaluate your system&apos;s responses?</li>
<li><strong>Personalization</strong>: How personalized does your model have to be? Do you need one model for all the users, for a group of users, or for each user individually? If you need multiple models, is it possible to train a base model on all the data and finetune it for each group or each user?</li>
<li><strong>Project constraints</strong>: These are the constraints that you have to worry about in the real world but less so during interviews: how much time you have until deployment, how much compute power is available, what kind of talents work on the project, what available systems can be used, etc.</li>
</ul>
<p><em>Resources</em></p>
<ul>
<li>Choosing the Right Metric for Evaluating Machine Learning Models by Alvira Swalin, USF-Data Science, 2018. <a href="https://medium.com/usf-msds/choosing-the-right-metric-for-machine-learning-models-part-1-a99d7d7414e4">Part I</a>. <a href="https://medium.com/usf-msds/choosing-the-right-metric-for-evaluating-machine-learning-models-part-2-86d5649a5428">Part II</a>.</li>
</ul>
</section><section data-type="sect1" id="data-pipeline-698WI4R"><h1>Data pipeline</h1>
<p>In school, you work with available, clean datasets and can spend most of your time on building and training machine learning models. In industry, you probably spend most of your time collecting, annotating, and cleaning data. When teaching, I noticed that many students shied away from data wrangling as they considered it uncool, the way a backend engineer sometimes considers frontend uncool, but the reality is that employers value highly both frontend and data wrangling abilities.</p>
<p>As machine learning is driven more by data than by algorithms, for every formulation of the problem that you propose, you should also tell your interviewer what kind of data and how much data you need: both for training and for evaluating your systems.</p>
<p>You need to specify the input and output of your system. There are many different ways to frame a problem. Consider the app prediction problem above. A naive setup would be to have a user profile (age, gender, ethnicity, occupation, income, technical savviness, etc.) and environment profile (time, location, previous apps used, etc.) as input and output a probability distribution for every single app available. This is a bad approach because there are too many apps and when a new app is added, you have to retrain your model. A better approach is to have the user profile, the environment, and the app profile as input, and output a binary classification whether it&apos;s a match or not.</p>
<p>Some of the questions you should ask your interviewer:</p>
<ul>
<li><strong>Data availability and collection</strong>: What kind of data is available? How much data do you already have? Is it annotated and if so, how good is the annotation? How expensive is it to get the data annotated? How many annotators do you need for each sample? How to resolve annotators&apos; disagreements? What&apos;s their data budget? Can you utilize any of the weakly supervised or unsupervised methods to automatically create new annotated data from a small amount of humanly annotated data?</li>
<li><strong>User data</strong>: What data do you need from users? How do you collect it? How do you get users&apos; feedback on the system, and if you want to use that feedback to improve the system online or periodically?</li>
<li><strong>Storage</strong>: Where is the data currently stored: on the cloud, local, or on the users&apos; devices? How big is each sample? Does a sample fit into memory? What data structures are you planning on using for the data and what are their tradeoffs? How often does the new data come in?</li>
<li><strong>Data preprocessing &amp; representation</strong>: How do you process the raw data into a form useful for your models? Will you have to do any featuring engineering or feature extraction? Does it need normalization? What to do with missing data? If there&apos;s class imbalance in the data, how do you plan on handling it? How to evaluate whether your train set and test set come from the same distribution, and what to do if they don&apos;t? If you have data of different types, say both texts, numbers, and images, how are you planning on combining them?</li>
<li><strong>Challenges</strong>: Handling user data requires extra care, as any of the many companies that have got into trouble for user data mishandling can tell you.</li>
<li><strong>Privacy</strong>: What privacy concerns do users have about their data? What anonymizing methods do you want to use on their data? Can you store users&apos; data back to your servers or can only access their data on their devices?</li>
<li><strong>Biases</strong>: What biases might represent in the data? How would you correct the biases? Are your data and your annotation inclusive? Will your data reinforce current societal biases?</li>
</ul>
<p><em>Resources</em></p>
<ul>
<li><a href="https://anand.typepad.com/datawocky/2008/03/more-data-usual.html">More data usually beats better algorithms</a> by Anand Rajaraman, Datawocky, 2008.</li>
</ul>
</section><section data-type="sect1" id="modeling-o97BIGR"><h1>Modeling</h1>
<p>Modeling, including model selection, training, and debugging, is what&apos;s often covered in most machine learning courses. However, it&apos;s only a small component of the entire process. Some might even argue that it&apos;s the easiest component.</p>
<center>
<figure>
<img alt="xkcd&apos;s modeling" src="assets/modeling.png" style="float: center; max-width: 45%; margin: 0 0 1em 1em">
<figcaption>Source: xkcd</figcaption>
</figure>
</center>
<section data-type="sect2" id="model-selection-eRQEIDR"><h2>Model selection</h2>
<p>Most problems can be framed as one of the common machine learning tasks, so familiarity with common machine learning tasks and the typical approaches to solve them will be very useful. You should first figure out the category of the problem. Is it supervised or unsupervised? Is it regression or classification? Does it require generation or only prediction? If generation, your models will have to learn the latent space of your data, which is a much harder task than just prediction.</p>
<p>Note that these &quot;or&quot; aren&apos;t mutually exclusive. An income prediction task can be regression if we output raw numbers, but if we quantize the income into different brackets and predict the bracket, it becomes a classification problem. Similarly, you can use unsupervised learning to learn labels for your data, then use those labels for supervised learning.</p>
<p>Then you can frame the question as a specific task: object recognition, text classification, time series analysis, recommender systems, dimensionality reduction, etc. Keep in mind that there are many ways to frame a problem, and you might not know which way works better until you&apos;ve tried to train some models.</p>
<p>When searching for a solution, your goal isn&apos;t to show off your knowledge of the latest buzzwords but to use the simplest solution that can do the job. Simplicity serves two purposes. First, gradually adding more complex components makes it easier to debug step by step. Second, the simplest model serves as a baseline to which you can compare your more complex models.</p>
<p>Setting up an appropriate baseline is an important step that many candidates forget. There are three different baselines that you should think about:</p>
<ul>
<li><em>Random baseline</em>: if your model just predicts everything at random, what&apos;s the expected performance?</li>
<li><em>Human baseline</em>: how well would humans perform on this task?</li>
<li><em>Simple heuristic</em>: for example, for the task of recommending the app to use next on your phone, the simplest model would be to recommend your most frequently used app. If this simple heuristic can predict the next app accurately 70% of the time, any model you build has to outperform it significantly to justify the added complexity.</li>
</ul>
<p>Your first step to approaching any problem is to find its effective heuristics. Martin Zinkevich, a research scientist at Google, explained in his handbook <em><a href="http://martin.zinkevich.org/rules_of_ml/rules_of_ml.pdf">Rules of Machine Learning: Best Practices for ML Engineering</a></em> that &quot;<em>if you think that machine learning will give you a 100% boost, then a heuristic will get you 50% of the way there.</em>&quot; However, resist the trap of increasingly complex heuristics. If your system has more than 100 nested if-else, it&apos;s time to switch to machine learning.</p>
<p>When considering machine learning models, don&apos;t forget that non-deep learning models exist. Deep learning models are often expensive to train and hard to explain. Most of the time, in production, they are only useful if their performance is unquestionably superior. For example, for the task of classification, before using a transformer-based model with 300 million parameters, see if a decision tree works. For fraud detection, before wielding complex neural networks, try one of the many popular non-neural network approaches such as k-nearest neighbor classifier.</p>
<p>Most real world problems might not even need deep learning. Deep learning needs data, and to gather data, you might first need users. To avoid the catch-22, you might want to launch your product without deep learning to gather user data to train your system.</p>
<p><em>Resources</em></p>
<ul>
<li><a href="https://blog.statsbot.co/machine-learning-algorithms-183cc73197c">Machine Learning Algorithms: Which One to Choose for Your Problem</a> by Daniil Korbut, Stats and Bots, 2017.</li>
</ul>
</section><section data-type="sect2" id="training-5RLqIW9"><h2>Training</h2>
<p>You should be able to anticipate what problems might arise during training and address them. Some of the common problems include: the training loss doesn&apos;t decrease, overfitting, underfitting, fluctuating weight values, dead neurons, etc. These problems are covered in the Regularization and training techniques, Optimization, and Activations sections in Chapter 9: Deep Learning.</p>
<section data-type="sect3" id="debugging-89pbIkl"><h3>Debugging</h3>
<p>Have you ever experienced the euphoria of having your model work flawlessly on the first run? Neither have I. Debugging a machine learning model is hard, so hard that poking fun at how incompetent we are at debugging machine learning models has become a sport.</p>
<center>
<figure>
<img alt="Debugging joke" src="assets/debugging.png" style="float: center; max-width: 50%; margin: 0 0 1em 1em">
<figcaption>Typo: idpb is supposed to be ipdb, python interactive debugger tool</figcaption>
</figure>
</center>
<p>There are many reasons that can cause a model to perform poorly:</p>
<ul>
<li><strong>Theoretical constraints</strong>: e.g. wrong assumptions, poor model/data fit.</li>
<li><strong>Poor model implementation</strong>: the more components a model has, the more things that can go wrong, and the harder it is to figure out which goes wrong.</li>
<li><strong>Snobby training techniques</strong>: e.g. call <code>model.train()</code> instead of <code>model.eval()</code> during evaluation.</li>
<li><strong>Poor choice of hyperparameters</strong>: with the same implementation, a set of hyperparameters can give you the state-of-the-art result but another set of hyperparameters might never converge.</li>
<li><strong>Data problems</strong>: mismatched inputs/labels, over-preprocessed data, noisy data, etc.</li>
</ul>
<p>Most of the bugs in deep learning are invisible. Your code compiles, the loss decreases, but your model doesn&apos;t learn anything or might never reach the performance it&apos;s supposed to. Having a procedure for debugging and having the discipline to follow that principle are crucial in developing, implementing, and deploying machine learning models.</p>
<p>During interviews, the interviewer might test your debugging skills by either giving you a piece of buggy code and ask you to fix it, or ask you about steps you&apos;d take to minimize the opportunities for bugs to proliferate. There is, unfortunately, still no scientific approach to debugging in machine learning. However, there have been a number tried-and-true debugging techniques published by experienced machine learning engineers and researchers. Here are some of the steps you can take to ensure the correctness of your model.</p>
<ol>
<li>
<p>Start simple and gradually add more components</p>
<p>Start with the simplest model and then slowly add more components to see if it helps or hurts the performance. For example, if you want to build a recurrent neural network (RNN), start with just one level of RNN cell before stacking multiple together, or adding more regularization. If you want to use a BERT-like model (<a href="https://arxiv.org/pdf/1810.04805.pdf">Devlin et al., 2018</a>) which uses both masked language model (MLM) and next sentence prediction loss (NSP), you might want to use only the MLM loss before adding NSP loss.</p>
<p>Currently, many people start out by cloning an open-source implementation of a state-of-the-art model and plugging in their own data. On the off-chance that it works, it&apos;s great. But if it doesn&apos;t, it&apos;s very hard to debug the system because the problem could have been caused by any of the many components in the model.</p>
</li>
<li>
<p>Overfit a single batch</p>
<p>After you have a simple implementation of your model, try to overfit a small amount of training data and run evaluation on the same data to make sure that it gets to the smallest possible loss. If it&apos;s for image recognition, overfit on 10 images and see if you can get to the accuracy to be 100%, or if it&apos;s for machine translation, overfit on 100 sentence pairs and see if you can get to the BLEU score of near 100. If it can&apos;t overfit a small amount of data, there&apos;s something wrong with your implementation.</p>
</li>
<li>
<p>Set a random seed</p>
<p>There are so many factors that contribute to the randomness of your model: weight initialization, dropout, data shuffling, etc. Randomness makes it hard to compare results across different  experiments -- you have no idea if the change in performance is due to a change in the model or a different random seed. Setting a random seed ensures consistency between different runs. It also allows you to reproduce errors and other people to reproduce your results.</p>
</li>
</ol>
<p><em>Resources</em></p>
<ul>
<li><a href="http://josh-tobin.com/assets/pdf/troubleshooting-deep-neural-networks-01-19.pdf">Troubleshooting Deep Neural Networks: A Field Guide to Fixing Your Model</a>. Josh Tobin, 2018.</li>
<li><a href="https://medium.com/infinity-aka-aseem/things-we-wish-we-had-known-before-we-started-our-first-machine-learning-project-336d1d6f2184">Things I wish we had known before we started our first Machine Learning project</a>. Aseem Bansal, towards-infinity, 2018.</li>
<li><a href="https://medium.com/@keeper6928/how-to-unit-test-machine-learning-code-57cf6fd81765">How to unit test machine learning code</a> by Chase Roberts, 2017</li>
<li><a href="http://karpathy.github.io/2019/04/25/recipe/">A Recipe for Training Neural Networks</a>. Andrej Karpathy, 2019.</li>
<li><a href="https://pcc.cs.byu.edu/2017/10/02/practical-advice-for-building-deep-neural-networks/">Practical Advice for Building Deep Neural Networks</a>. Matt Holt and Daniel Ricks, BYU&apos;s Perception, Control and Cognition Laboratory, 2017.</li>
<li><a href="https://medium.com/ai%C2%B3-theory-practice-business/top-6-errors-novice-machine-learning-engineers-make-e82273d394db">Top 6 errors novice machine learning engineers make</a>. Christopher Dossman, AI&#xB3; | Theory, Practice, Business, 2017.</li>
</ul>
</section><section data-type="sect3" id="hyperparameter-tuning-BlADIyw"><h3>Hyperparameter tuning</h3>
<p>With different sets of hyperparameters, the same model can give drastically different performance on the same dataset. Melis et al. showed in their 2018 paper <em><a href="https://arxiv.org/pdf/1707.05589.pdf">On the State of the Art of Evaluation in Neural Language Models</a></em> that weaker models with well-tuned hyperparameters can outperform stronger, more recent models.</p>
<p>Despite knowing its importance, people without real-world experience often ignore systematic approaches to hyperparameter tuning in favor of manual, gut-feeling approach. The most popular method is arguably Graduate Student Descent (GSD), a technique in which a graduate student plays around with the hyperparameters until the model works (GSD is a well-documented technique, see <a href="https://www.reddit.com/r/MachineLearning/comments/6hso7g/d_how_do_people_come_up_with_all_these_crazy_deep/dj0tz1c/">here</a>, <a href="https://www.reddit.com/r/MachineLearning/duplicates/8yvlzy/d_debate_about_science_at_organizations_like/">here</a>, <a href="https://sciencedryad.wordpress.com/2014/01/25/grad-student-descent/">here</a>, and <a href="https://twitter.com/guyzys/status/592847074170896384?lang=en">here</a>).</p>
<p>There have been a lot of research done on hyperparameter search algorithms, as well as tools to help you automatically search for a good set of hyperparameters. You might want to check out some of the popular methods for hyperparameter tuning including random search, grid search, Bayesian optimization. The book <em>AutoML: Methods, Systems, Challenges</em> by the AutoML group at the University of Freiburg dedicates its first chapter to hyperparameter optimization, which you can read online for free <a href="https://www.automl.org/wp-content/uploads/2018/09/chapter1-hpo.pdf">here</a>.</p>
<p>The performance of each set of hyperparameters is evaluated on the validation set. Keep in mind that not all hyperparameters are created equal. A model&apos;s performance might be more sensitive to the change in one hyperparameter, and there have also been research done on accessing the importance of different hyperparameters.</p>
</section><section data-type="sect3" id="scaling-49BpIQl"><h3>Scaling</h3>
<p>As models are getting bigger and more resource-intensive, companies care a lot more about training at scale. It&apos;s usually not listed as requirements since expertise in scalability is hard to acquire without regular access to massive compute resources. For machine learning engineering roles, you&apos;ll get huge bonus points if you&apos;re familiar with common scalability challenges and solutions. Scalability is an elaborate topic that merits its own book. This section covers some common issues, but scratches only the surface.</p>
<p>It&apos;s not uncommon to train a model with a dataset that can&apos;t be fit into the main memory. This is especially common when dealing with medical data such as CT scans or genome sequences. If you run into a situation like this, you should know how to preprocess (e.g. zero-centering, normalizing, whitening), shuffle, and batch your data when it doesn&apos;t fit into memory. When each sample of your data is too large, your model can handle a very small batch size, which can lead to instability for stochastic gradient descent based optimization.</p>
<p>On a very rare case, each sample is so large a single sample can&apos;t even fit into the memory, you will have to use techniques such as gradient checkpointing, a technique that leverages the memory footprint/computation tradeoff to make your system do more computation but require less memory. You can use an open-source package <a href="https://github.com/cybertronai/gradient-checkpointing"><code>gradient-checkpointing</code></a> developed by by Tim Salimans and Yaroslav Bulatov. According to the authors of the package, &quot;<em>for feed-forward model, we were able to fit more than 10x larger models onto our GPU, at only a 20% increase in computation time.</em>&quot;</p>
<p>It&apos;s almost the norm now for machine learning engineers and researchers to train their models on multiple machines (CPUs, GPUs, TPUs). Modern machine learning frameworks make it easy to do distributed training. The most common parallelization method with multiple workers is data parallelism: you split your data on multiple machines, train your model on all of them, and accumulate gradients. This gives rise to a couple of issues.</p>
<p>The most challenging problem is how to accurately and effectively accumulate gradients from different machines. As each machine produces its own gradient, if your model waits for all of them to finish a run -- this technique is called Synchronous stochastic gradient descent (SSGD) -- stragglers will cause the entire model to slow down.</p>
<p>However, if your model updates the weight using gradient from each machine separately -- this is called Asynchronous SGD (ASGD) -- it will cause gradient staleness because the gradients from one machine has caused the weights to change before the gradients from another machine has come in. How to mitigate gradient staleness is an active area of research.</p>
<p>Second, spreading your model on multiple machines can cause your batch size to be very big. If a machine processes a batch of size 128, then 128 machines processes a batch of size 16,384. If training an epoch on a machine takes 100k steps, training on 128 machines takes under 800 steps. An intuitive approach is to scale learning rate on multiple machines to account for so much more learning at each step, but we also can&apos;t make the learning rate too big as it will lead to unstable convergence.</p>
<p>Last but not least, with the same model setup, the master worker will use a lot more resources than other workers. To make the most use out of all machines, you need to figure out a way to balance out the workload among them. The easiest way, but not the most effective way, is to use a smaller batch size on the master worker and a larger batch size on other workers.</p>
<p>With data parallelism, each worker has its own copy of the model and does all the computation necessary for the model. Model parallelism is when different components of your model can be evaluated on different machines. For example, machine 0 handles the computation for the first two layers while machine 1 handles the next two layers, or some machines can handle the forward pass while several others handle the backward pass. In theory, nothing stops you from using both data parallelism and model parallelism. However, in practice, it can pose a massive engineering challenge.</p>
<p>A scaling approach that has gained increasing popularity is to reduce the precision during training. Instead of using a full 32 bits to represent a floating point number, you can use less bits for each number while maintaining a model&apos;s predictive power. The paper <em><a href="https://arxiv.org/abs/1710.03740">Mixed Precision Training</a></em> by Paulius Micikevicius et al. at NVIDIA showed that by alternating between full floating point precision (32 bits) and half floating point precision (16 bits), we can reduce the memory footprint of a model by half, which allows us to double our batch size. Less precision also speeds up computation.</p>
<p>Most modern hardwares for deep learning take advantage of mixed and/or reduced precision training. Newer NVIDIA GPUs, such as Volta and Turing architecture, feature Tensor Cores, processing units that support mixed precision training. <a href="https://devblogs.nvidia.com/mixed-precision-nlp-speech-openseq2seq/">Compared to standard FP32 on P100, Tensor Cores provide up to 12x higher peak TFLOPS during training, and up to 6x during inferencing</a>. Google TPUs also support training with Bfloat16 (16-bit Brain Floating Point Format), which the company dubbed as &quot;<em><a href="https://cloud.google.com/blog/products/ai-machine-learning/bfloat16-the-secret-to-high-performance-on-cloud-tpus">the secret to high performance on Cloud TPUs.</a></em>&quot;</p>
<p><em>Resources</em></p>
<ul>
<li><a href="https://medium.com/huggingface/training-larger-batches-practical-tips-on-1-gpu-multi-gpu-distributed-setups-ec88c3e51255">Training Neural Nets on Larger Batches: Practical Tips for 1-GPU, Multi-GPU &amp; Distributed setups</a>. Thomas Wolf. 2018.</li>
<li><a href="https://arxiv.org/pdf/1802.09941.pdf">Demystifying Parallel and Distributed Deep Learning: An In-Depth Concurrency Analysis</a>. Tal Ben-Nun and Torsten Hoefler. 2018.</li>
<li><a href="https://medium.com/hackernoon/a-guide-to-scaling-machine-learning-models-in-production-aa8831163846">A Guide to Scaling Machine Learning Models in Production</a>. Hamza Harkous, Hackernoon. 2017.</li>
<li><a href="https://www2.eecs.berkeley.edu/Pubs/TechRpts/2017/EECS-2017-156.pdf">Scaling SGD Batch Size to 32K for ImageNet Training</a>. Yang You, Igor Gitman, and Boris Ginsburg. Berkeley EECS. 2018.</li>
</ul>
</section></section></section><section data-type="sect1" id="serving-091rIYw"><h1>Serving</h1>
<p>Before serving your trained models to users, you need to think of experiments you need to run to make sure that your models meet all the constraints outlined in the problem setup. You need to think of what feedback you&apos;d like to get from your users, whether to allow users to suggest better predictions, and from user reactions, how to defer whether your model does a good job.</p>
<p>Training and serving aren&apos;t two isolated processes. Your model will continuously improve as you get more user feedback. Do you want to train your model online with each new data point? Do you need to personalize your model to each user? How often should you update your machine learning model?</p>
<p>Some changes to your model require more effort than others. If you want to add more training samples, you can continue training your existing model on the new samples. However, if you want to add a new label class to a neural classification model, you&apos;re likely need to retrain the entire system.</p>
<p>If it&apos;s a prediction model, you might want to measure your model&apos;s confidence with each prediction so that you can show only predictions that your model is confident about. You might also want to think about what to do in case of low confidence -- e.g. would you refer your user to a human specialist or collect more data from them?</p>
<p>You should also think about how to run inferencing: on the user device or on the server and the tradeoffs between them. Inferencing on the user phone consumes the phone&apos;s memory and battery, and makes it harder for you to collect user feedback. Inferencing on the cloud increases the product latency, requires you to set up a server to process all user requests, and might scare away privacy-conscious users.</p>
<p>And there&apos;s the question of interpretability. If your model predicts that someone shouldn&apos;t get a loan, that person deserves to know the reason why. You need to consider the performance/interpretability tradeoffs. Making a model more complex might increase its performance but make the results harder to interpret.</p>
<p>For complex models with many different components, it&apos;s especially important to conduct ablation studies -- removing each component while keeping the rest --  to determine the efficiency of each component. You might find components whose removals don&apos;t significantly reduce the model&apos;s performance but significantly reduce its complexity.</p>
<p>You also need to think about the potential biases and misuses of your model. Does it propagate any gender and racial biases from the data, and if so, how will you fix it? What happens if someone with malicious intent has access to your system?</p>
<p>On the engineering side, there are many challenges involved in deploying a machine learning model. However, most companies likely have their own deployment teams who know a lot about deployment and less about machine learning.</p>
<hr>
<p><strong><em>Note</em></strong>: The assumptions your model is making</p>
<p>The statistician George Box said in 1976 that &quot;<em>all models are wrong, but some are useful.</em>&quot; The real world is intractably complex, and models can only approximate using assumptions. Every single model comes with its own assumptions. It&apos;s important to think about what assumptions your model makes and whether our data satisfies those assumptions.</p>
<p>Below are some of the common assumptions. It&apos;s not meant to be an exhaustive list, but just a demonstration.</p>
<ul>
<li>Prediction assumption: every model that aims to predict an output Y from an input X makes the assumption that it&apos;s possible to predict Y based on X.</li>
<li>IID: Neural networks assumes that the data points are independent and identically distributed.</li>
<li>Smoothness: Every supervised machine learning method assumes that there&apos;s a set of functions that can transform inputs into outputs such that similar inputs are transformed into similar outputs. If an input X produces an output Y, then an input close to X would produce an output proportionally close to Y.</li>
<li>Tractability: Let X be the input and Z be the latent representation of X. Every generative model makes the assumption that it&apos;s tractable to compute the probability P(Z | X).</li>
<li>Boundaries: A linear classifier assumes that decision boundaries are linear.</li>
<li>Conditional independence: A Naive Bayes classifier assumes that the attribute values are independent of each other given the class.
Normally distributed: many statistical methods assume that data is normally distributed.</li>
</ul>
<hr>
<br>
<hr>
<p><strong><em>Note</em></strong>: Tips on preparing</p>
<p>The list of steps above is long and intimidating. Think of a project you did in the past and try to answer the following questions.</p>
<ul>
<li>How did you collect the data? How did you process your data?</li>
<li>How did you decide what models to use? What models did eventually try? What models did better? Why? Any surprise?</li>
<li>How did you evaluate your models?</li>
<li>If you did the project again, what would you do differently?</li>
</ul>
<hr>
<p><em>Resources</em></p>
<ul>
<li><a href="http://martin.zinkevich.org/rules_of_ml/rules_of_ml.pdf">Rules of Machine Learning: Best Practices for ML Engineering</a>. Martin Zinkevich, 2017.</li>
<li><a href="https://towardsdatascience.com/architecting-a-machine-learning-pipeline-a847f094d1c7">How to build scalable Machine Learning systems - Part II: Architecting a Machine Learning Pipeline</a>. Semi Koen, Towards Data Science, 2017.</li>
<li><a href="https://medium.com/@Zelros/a-brief-history-of-machine-learning-models-explainability-f1c3301be9dc">A Brief History of Machine Learning Models Explainability</a>. Zelros AI, 2018.</li>
<li><a href="https://img1.wsimg.com/blobby/go/3d82daa4-97fe-4096-9c6b-376b92c619de/downloads/MaliciousUseofAI.pdf">The Malicious Use of Artificial Intelligence: Forecasting, Prevention, and Mitigation</a>. Miles Brundage et al., 2018.</li>
<li><a href="https://developers.google.com/machine-learning/crash-course/fairness/video-lecture">Fairness in Machine Learning Engineering crash course</a>. Google.</li>
</ul>
</section></section>
<section data-type="chapter" id="case-studies-bYrWS80"><h1>Case studies</h1>
<p>To learn to design machine learning systems, it&apos;s helpful to read case studies to see how actual teams deal with different deployment requirements and constraints. Many companies, such as Airbnb, Lyft, Uber, Netflix, run excellent tech blogs where they share their experience using machine learning to improve their products and/or processes. If you&apos;re interested in a company, you should visit their tech blogs to see what they&apos;ve been working on -- it might come up during your interviews! Below are some of these great case studies.</p>
<ol>
<li>
<p><a href="https://medium.com/airbnb-engineering/using-machine-learning-to-predict-value-of-homes-on-airbnb-9272d3d4739d">Using Machine Learning to Predict Value of Homes On Airbnb</a> (Robert Chang, Airbnb Engineering &amp; Data Science, 2017)</p>
<p>In this detailed and well-written blog post, Chang described how Airbnb used machine learning to predict an important business metric: the value of homes on Airbnb. It walks you through the entire workflow: feature engineering, model selection, prototyping, moving prototypes to production. It&apos;s completed with lessons learned, tools used, and code snippets too.</p>
</li>
<li>
<p><a href="https://medium.com/netflix-techblog/using-machine-learning-to-improve-streaming-quality-at-netflix-9651263ef09f">Using Machine Learning to Improve Streaming Quality at Netflix</a> (Chaitanya Ekanadham, Netflix Technology Blog, 2018)</p>
<p>As of 2018, Netflix streams to over 117M members worldwide, half of those living outside the US. This blog post describes some of their technical challenges and how they use machine learning to overcome these challenges, including to predict the network quality, detect device anomaly, and allocate resources for predictive caching.</p>
</li>
<li>
<p><a href="https://blog.acolyer.org/2019/10/07/150-successful-machine-learning-models/">150 Successful Machine Learning Models: 6 Lessons Learned at Booking.com</a> (Bernardi et al., KDD, 2019).</p>
<p>As of 2019, Booking.com has around 150 machine learning models in production. These models solve a wide range of prediction (e.g. predicting users&apos; travel preferences and how many people they travel with) and optimization (e.g.optimizing the background images and reviews to show for each user). Adrian Colyer gave a good summary of the six lessons learned here:</p>
<ul>
<li>Machine learned models deliver strong business value.</li>
<li>Model performance is not the same as business performance.</li>
<li>Be clear about the problem you&apos;re trying to solve.</li>
<li>Prediction serving latency matters.</li>
<li>Get early feedback on model quality.</li>
<li>Test the business impact of your models using randomized controlled trials.</li>
</ul>
</li>
<li>
<p><a href="https://medium.com/hackernoon/how-we-grew-from-0-to-4-million-women-on-our-fashion-app-with-a-vertical-machine-learning-approach-f8b7fc0a89d7">How we grew from 0 to 4 million women on our fashion app, with a vertical machine learning approach</a> (Gabriel Aldamiz, HackerNoon, 2018)</p>
<p>To offer automated outfit advice, Chicisimo tried to qualify people&apos;s fashion taste using machine learning. Due to the ambiguous nature of the task, the biggest challenges are framing the problem and collecting the data for it, both challenges are addressed by the article. It also covers the problem that every consumer app struggles with: user retention.</p>
</li>
<li>
<p><a href="https://medium.com/airbnb-engineering/machine-learning-powered-search-ranking-of-airbnb-experiences-110b4b1a0789">Machine Learning-Powered Search Ranking of Airbnb Experiences</a> (Mihajlo Grbovic, Airbnb Engineering &amp; Data Science, 2019)</p>
<p>This article walks you step by step through a canonical example of the ranking and recommendation problem. Four main steps are system design, personalization, online scoring, and business aspect. The article explains which features to use, how to collect data and label it, why they chose Gradient Boosted Decision Tree, which testing metrics to use, what heuristics to take into account while ranking results, how to do A/B testing during deployment. Another wonderful thing about this post is that it also covers personalization to rank results differently for different users.</p>
</li>
<li>
<p><a href="https://eng.lyft.com/from-shallow-to-deep-learning-in-fraud-9dafcbcef743">From shallow to deep learning in fraud</a> (Hao Yi Ong, Lyft Engineering, 2018)</p>
<p>Fraud detection is one of the earliest use cases of machine learning in industry. This article explores the evolution of fraud detection algorithms used at Lyft. At first, an algorithm as simple as logistic regression with engineered features was enough to catch most fraud cases. Its simplicity allowed the team to understand the importance of different features. Later, when fraud techniques have become too sophisticated, more complex models are required. This article explores the tradeoff between complexity and interpretability, performance and ease of deployment.</p>
</li>
<li>
<p><a href="https://tech.instacart.com/space-time-and-groceries-a315925acf3a">Space, Time and Groceries</a> (Jeremy Stanley, Tech at Instacart, 2017)</p>
<p>Instacart uses machine learning to solve the task of path optimization: how to most efficiently assign tasks for multiple shoppers and find the optimal paths for them.  The article explains the entire process of system design, from framing the problem, collecting data, algorithm and metric selection, topped with tutorial for beautiful visualization.</p>
</li>
<li>
<p><a href="https://eng.uber.com/uber-big-data-platform/">Uber&apos;s Big Data Platform: 100+ Petabytes with Minute Latency</a> (Reza Shiftehfar, Uber Engineering, 2018)</p>
<p>With massive data comes massive engineering requirement. Relying heavily on data for decision making, &quot;from forecasting rider demand during high traffic events to identifying and addressing bottlenecks in our driver-partner sign-up process&quot;, Uber has collected &quot;over 100 petabytes of data that needs to be cleaned, stored, and served with minimum latency.&quot; This article focuses on the evolution of analytical data warehouse at Uber, from Vertica to Hadoop to their own Spark library Hudi, each with their limitations analyzed and addressed.</p>
</li>
<li>
<p><a href="https://blogs.dropbox.com/tech/2017/04/creating-a-modern-ocr-pipeline-using-computer-vision-and-deep-learning/">Creating a Modern OCR Pipeline Using Computer Vision and Deep Learning</a> (Brad Neuberg, Dropbox Engineering, 2017)</p>
<p>An application as simple as a document scanner has two distinct components: optical character recognition and word detector. Each requires their own production pipeline, and the end-to-end system requires additional steps for training and tuning. This article also goes into detail the team&apos;s effort to collect data, which includes building their own data annotation platform.</p>
</li>
<li>
<p><a href="https://eng.uber.com/scaling-michelangelo/">Scaling Machine Learning at Uber with Michelangelo</a> (Jeremy Hermann and Mike Del Balso, Uber Engineering, 2019)</p>
<p>Uber uses extensive machine learning in their production, and this article gives an impressive overview of their end-to-end workflow, where machine learning is being applied at Uber, and how their teams are organized.</p>
</li>
</ol>
</section>
<section data-type="chapter" id="exercises-rWl8SQW"><h1>Exercises</h1>
<p>The answers for these questions will be published in the book <strong>Machine Learning Interviews</strong>. You can look at and contribute to community answers to these questions on GitHub <a href="https://github.com/chiphuyen/machine-learning-systems-design/tree/master/answers">here</a>. You can read more about the book and sign up for the book&apos;s mailing list <a href="https://huyenchip.com/2019/07/21/machine-learning-interviews.html">here</a>.</p>
<p>Note: many questions are ambiguous on purpose. It&apos;s your job, as a candidate, to ask for clarification and narrow down the scope of the problem.</p>
<ol>
<li>Duolingo is a platform for language learning. When a student is learning a new language, Duolingo wants to recommend increasingly difficult stories to read.
<ul>
<li>How would you measure the difficulty level of a story?</li>
<li>Given a story, how would you edit it to make it easier or more difficult?</li>
</ul>
</li>
<li>Given a dataset of credit card purchases information, each record is labelled as fraudulent or safe, how would you build a fraud detection algorithm?</li>
<li>You run an e-commerce website. Sometimes, users want to buy an item that is no longer available. Build a recommendation system to suggest replacement items.</li>
<li>For any user on Twitter, how would you suggest who they should follow? What do you do when that user is new? What are some of the limitations of data-driven recommender systems?</li>
<li>When you enter a search query on Google, you&apos;re shown a list of related searches. How would you generate a list of related searches for each query?</li>
<li>Build a system that return images associated with a query like in Google Images.</li>
<li>How would you build a system to suggest trending hashtags on Twitter?</li>
<li>Each question on Quora often gets many different answers. How do you create a model that ranks all these answers? How computationally intensive is this model?</li>
<li>How to you build a system to display top 10 results when a user searches for rental listings in a certain location on Airbnb?</li>
<li>Autocompletion: how would you build an algorithm to finish your sentence when you text?</li>
<li>When you type a question on StackOverflow, you&apos;re shown a list of similar questions to make sure that your question hasn&apos;t been asked before. How do you build such a system?</li>
<li>How would you design an algorithm to match pool riders for Lyft or Uber?</li>
<li>On social networks like Facebook, users can choose to list their high schools. Can you estimate what percentage of high schools listed on Facebook are real? How do we find out, and deploy at scale, a way of finding invalid schools?</li>
<li>How would you build a trigger word detection algorithm to spot the word &quot;activate&quot; in a 10 second long audio clip?</li>
<li>If you were to build a Netflix clone, how would you build a system that predicts when a user stops watching a TV show, whether they are tired of that show or they&apos;re just taking a break?</li>
<li>Facebook would like to develop a way to estimate the month and day of people&apos;s birthdays, regardless of whether people give us that information directly. What methods would you propose, and data would you use, to help with that task?</li>
<li>Build a system to predict the language a text is written in.</li>
<li>Predict the house price for a property listed on Zillow. Use that system to predict whether we invest on buying more properties in a certain city.</li>
<li>Imagine you were working on iPhone. Everytime users open their phones, you want to suggest one app they are most likely to open first with 90% accuracy. How would you do that?</li>
<li>How do you map nicknames (Pete, Andy, Nick, Rob, etc) to real names?</li>
<li>An e-commerce company is trying to minimize the time it takes customers to purchase their selected items. As a machine learning engineer, what can you do to help them?</li>
<li>Build a chatbot to help people book hotels.</li>
<li>How would you design a question answering system that can extract an answer from a large collection of documents given a user query?</li>
<li>How would you train a model to predict whether the word &quot;jaguar&quot; in a sentence refers to the animal or the car?</li>
<li>Suppose you&apos;re building a software to manage the stock portfolio of your clients. You manage X amount of money. Imagine that you&apos;ve converted all that amount into stocks, and find a stock that you definitely must buy. How do you decide which of your currently owned stocks to drop so that you can buy this new stock?</li>
<li>How would you create a model to recognize whether an image is a triangle, a circle, or a square?</li>
<li>Given only CIFAR-10 dataset, how to build a model to recognize if an image is in the 10 classes of CIFAR-10 or not?</li>
</ol>
</section>
  </body>
</html>
